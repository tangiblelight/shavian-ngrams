#!/bin/python3
"""
Runs standard input through a part-of-speech tagger, then
translates to Shavian. This resolves most heteronyms, but
do still check the output for @ signs and fix them by hand.

Each line of a dictionary consists of an English word, a space,
a Shavian translation, and no comments. Special notations are:

^word ğ‘¢ğ‘»ğ‘›	word is a prefix
$word ğ‘¢ğ‘»ğ‘›	word is a suffix
Word ğ‘¢ğ‘»ğ‘›	always use a namer dot
word_ ğ‘¢ğ‘»ğ‘›	never use a namer dot
word_VB ğ‘¢ğ‘»ğ‘›	shave this way when tagged as a verb
word. ğ‘¢ğ‘»ğ‘›	shave this way when no suffix is present
word .ğ‘¢ğ‘»ğ‘›	word takes no prefixes
word ğ‘¢ğ‘»ğ‘›.	word takes no suffixes
word ğ‘¢ğ‘»ğ‘›:	suffixes do not alter the root,
	      	e.g. "ğ‘‘ğ‘¾" or "ğ‘•ğ‘¾" palatizing to "ğ‘–ğ‘©" or "ğ‘ ğ‘©".
word .		delete this word from the dictionary

Words are matched case-sensitive when possible, e.g. US/us,
WHO/who, Job/job, Nice/nice, Polish/polish.

shaw.py does not care about the order of dictionary entries.
shaw.c requries a highly specific order not described here.
"""
import importlib.resources
import re
import os
import sys
import html
from html.parser import HTMLParser

import pandas as pd

apostrophe = "'"  # whatever you want for apostrophe, e.g. "â€™" or ""
dot_entire_name = True
if os.path.exists("config.py"):
    from config import *

script = flair = spaCy = notags = False
dict = htags = {}
tokens = ["."]
units = {"ms": "ğ‘¥ğ‘•", "bc": "ğ‘šğ‘°ğ‘•ğ‘°", "psi": "ğ‘ğ‘•ğ‘¦", "pc": "ğ‘ğ‘•", "mi": "ğ‘¥ğ‘²"}
contr = ["'d", "'ll", "'m", "n't", "'re", "'s", "'ve"]
abbrev = [
    "abbr",
    "acad",
    "al",
    "alt",
    "apr",
    "assn",
    "at",
    "aug",
    "ave",
    "b",
    "c",
    "cf",
    "capt",
    "cent",
    "chm",
    "chmn",
    "co",
    "col",
    "comdr",
    "corp",
    "cpl",
    "d",
    "dec",
    "dept",
    "dist",
    "div",
    "dr",
    "ed",
    "esq",
    "est",
    "etc",
    "feb",
    "fl",
    "gen",
    "gov",
    "hon",
    "inc",
    "inst",
    "jan",
    "jr",
    "lat",
    "lib",
    "lt",
    "ltd",
    "mar",
    "mr",
    "mrs",
    "ms",
    "msgr",
    "mt",
    "mts",
    "mus",
    "nov",
    "oct",
    "pg",
    "phd",
    "pl",
    "pop",
    "pp",
    "prof",
    "pseud",
    "pt",
    "rev",
    "sept",
    "ser",
    "sgt",
    "sr",
    "st",
    "uninc",
    "univ",
    "vol",
    "vs",
    "wt",
]


# Remove diacritics from Latin letters, break up ligatures, and do nothing else.
def unaccent(str):
    map = "AAAAAA CEEEEIIIIDNOOOOO OUUUUY  aaaaaa ceeeeiiiidnooooo ouuuuy yAaAaAaCcCcCcCcDdDdEeEeEeEeEeGgGgGgGgHhHhIiIiIiIiIi  JjKkkLlLlLlLlLlNnNnNn   OoOoOo  RrRrRrSsSsSsSsTtTtTtUuUuUuUuUuUuWwYyYZzZzZz bBBb   CcDDDdd  EFfG  IIKkl  NnOOo  Pp     tTtTUuYVYyZz    255      Ç±Ç²Ç³      AaIiOoUuUuUuUuUu AaAaÃ†Ã¦GgGgKkOoOo  j   Gg  NnAaÃ†Ã¦OoAaAaEeEeIiIiOoOoRrRrUuUuSsTt  HhNd  ZzAaEeOoOoOoOoYylntj  ACcLTsz  BU EeJjqqRrYy"
    lig = {
        "Ã†": "AE",
        "Ã¦": "ae",
        "Ç±": "DZ",
        "Ç²": "Dz",
        "Ç³": "dz",
        "Ä²": "Ij",
        "Ä³": "ij",
        "Ç‡": "LJ",
        "Çˆ": "Lj",
        "Ç‰": "lj",
        "ÇŠ": "NJ",
        "Ç‹": "Nj",
        "ÇŒ": "nj",
        "Å’": "OE",
        "Å“": "oe",
        "Æ¢": "OI",
        "Æ£": "oi",
        "ÃŸ": "ss",
        "ï¬€": "ff",
        "ï¬": "fi",
        "ï¬‚": "fl",
        "ï¬ƒ": "ffi",
        "ï¬„": "ffl",
        "ï¬…": "st",
        "ï¬†": "st",
    }
    ret = ""
    for char in str:
        n = ord(char)
        if n >= 0xC0 and n < 0x250 and map[n - 0xC0] != " ":
            char = map[n - 0xC0]
        if n >= 0x300 and n < 0x370:
            char = ""
        if char in lig:
            char = lig[char]
        ret += char
    return ret


def notrans(str):
    global htags
    if toki in htags:
        htags[toki] += str
    else:
        htags[toki] = str


def tokenize(str):
    global tokens, toki
    str = " " + unaccent(html.unescape(str)) + " "
    old = 0
    for i in range(1, len(str) - 1):
        new = 0
        if str[i].isalpha():
            new = 1
        if str[i].isdigit():
            new = 2
        if str[i] == " " and tokens[-1][0].isalpha() and str[i + 1].isalpha():
            new = 0
        if str[i] in "'." and str[i - 1].isalpha() and str[i + 1].isalpha():
            new = 1
        if str[i] in ",." and str[i - 1].isdigit() and str[i + 1].isdigit():
            new = 2
        if str[i] == "." and old == 1 and "." in tokens[-1]:
            new = 1  # U.S.A. keeps the dot
        if str[i] == "." and new == 0 and tokens[-1].lower() in abbrev:
            continue  # Dr. Mr. etc. lose it
        if old and old == new:
            tokens[-1] += str[i]
        else:
            for c in contr:  # break up contractions so PoS tagging works
                s = len(tokens[-1]) - len(c)
                if s < 1:
                    continue
                low = tokens[-1][s:].lower()
                if c == low:
                    tokens[-1] = tokens[-1][:s]
                    tokens.append(low)
            tokens.append(str[i])
            toki += 1
        old = new
        if (
                tokens[-1].isspace()
                or not tokens[-1].isprintable()
                or ord(tokens[-1][0]) | 15 == 0xFE0F
        ):
            toki -= 1  # Whitespace tokens break NLTK and variation
            notrans(tokens.pop())  # selectors break Flair. Move these to htags.


class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        global script
        out = "<" + tag
        for at in attrs:
            if at[0] == "charset":
                at = ("charset", "UTF-8")
            if at[0] == "content":
                at = ("content", "text/html; charset=UTF-8")
            out += " " + at[0]
            if type(at[1]) == str:
                out += '="' + at[1] + '"'
        out += ">"
        if tag == "noscript" or tag == "script" or tag == "style":
            script = True
        notrans(out)

    def handle_endtag(self, tag):
        global script
        notrans("</" + tag + ">")
        if tag == "noscript" or tag == "script" or tag == "style":
            script = False

    def handle_data(self, data):
        if script:
            notrans(data)
        else:
            tokenize(data)


# Search all the ways a word might appear in the dictionary
def lookup(word, pos):
    if not aflag and re.fullmatch("[WMDC]*[CLX]*[XVI]*", word) and word != "I":
        return word
    ret = ""
    low = word.lower()
    upp = word[0].upper() + low[1:]
    pos = "_" + pos
    if aflag & 2:
        list = [
            low + pos,
            low + pos[:3],
            word,
            word + "_",
            low,
            low + "_",
            low + "_NN",
            low + "_NNS",
            upp,
        ]
    else:
        list = [
            low + pos,
            low + pos[:3],
            word + ".",
            upp + ".",
            word,
            word + "_",
            low + ".",
            low,
            low + "_",
            low + "_NN",
            low + "_NNS",
            upp,
        ]
    for look in list:
        if look in dict:
            ret = dict[look]
            if aflag & 1 and ret[0] == "." or aflag & 2 and ret[-1] == ".":
                ret = ""
            ret = ret.replace(".", "")
            if not ret:
                continue
            if (
                    (word[0].isupper() or look[0].isupper())
                    and (look[-1] != "_" or aflag)
                    and not re.search("[A-Z]", ret)
            ):
                ret = "Â·" + ret
            break
    return ret


def suffix_split(inp, pos, adj):
    global aflag
    long = len(inp)
    root = lookup(inp, pos)
    if root:
        return ((long + adj) ** 2, root)
    low = inp.lower()
    best = (0, "")
    for split in range(2, long):
        suff = "$" + low[split:]
        if not suff in dict:
            continue
        if low[split] == low[split - 1]:
            if long - split == 1 or low[split] in "eos":
                continue
        if low[split:] == "es" and low[split - 1] not in "hiosuxz":
            continue
        if low[split:] == "ry" and low[split - 1] in "aeiouf":
            continue
        if low[split:] == "ha" and low[split - 1] in "cst":
            continue
        if low[split:] == "th" and low[split - 1] in "e":
            continue
        if low[split:] == "t" and low[split - 1] in "aeioust'":
            continue
        if low[split:] == "k" and low[split - 1] in "aceino":
            continue
        if low[split:] == "r" and low[split - 1] in "aeiou":
            continue
        if low[split:] == "m" and low[split - 1] in "eis":
            continue
        if low[split:] == "z" and low[split - 1] in "i":
            continue
        if (
                low[split:] == "n"
                and low[split - 1] in "eio"
                and low[split - 2] != low[split - 1]
        ):
            continue
        if (
                low[split:] == "ess"
                and low[split - 1] in "ln"
                and low[split - 2] == low[split - 1]
        ):
            continue
        suff = dict[suff]
        for pess in range(2):
            if (pess) or inp[split] == "'":
                word = inp[:split]
            elif low[split - 1] == "i" and low[split] not in "cfikmpsv":
                word = inp[: split - 1] + "y"
            elif (
                    low[split] in "aeiouy"
                    and low[split - 1] not in "aeio"
                    and low[split: split + 2] not in ["ub", "up"]
            ):
                if low[split - 1] == low[split - 2] and low[split - 1] not in "hsw":
                    word = inp[: split - 1]
                elif (
                        low[split - 1] in "cghlsuvz"
                        or low[split] == "e"
                        or low[split - 2] in "aeiousy"
                ) and (low[split - 1] not in "cg" or low[split] not in "aou"):
                    word = inp[:split] + "e"
                else:
                    continue
            elif low[split - 2: split] == "dg":
                word = inp[:split] + "e"
            else:
                continue
            aflag &= ~2
            if inp[split] != "'":
                aflag |= 2
            root = suffix_split(word, "UNK", split - len(word))
            score = (long - split + adj) ** 2 + root[0] if root[0] else 0
            if score:
                if low[split:] in ["call"]:
                    score = 1
                if low[split:] in [
                    "bed",
                    "can",
                    "cent",
                    "dance",
                    "den",
                    "ine",
                    "kin",
                    "one",
                    "pal",
                    "path",
                    "ster",
                    "wing",
                    "x",
                ]:
                    score = max(1, score - 9)
            if score <= best[0]:
                continue
            root = root[1]
            if (
                    low[split - 1] == "e"
                    and low[split - 2] not in "aegiou"
                    and low[split] in "aou"
                    and (split + 1 == long or low[split + 1] in "dlmnprstu")
                    and low[split:] not in ["arm", "out", "und", "up"]
            ):
                if root[-1] in "ğ‘¦ğ‘°":
                    root = root[:-1]
                root += "ğ‘¦"
            if (
                    root[-1] == "ğ‘“"
                    and suff[0] > "ğ‘—"
                    and word[-1] in "vw"
                    and low[split:] != "s"
            ):
                root = root[:-1] + "ğ‘"
            if root[-2:] == "ğ‘©ğ‘¤" and suff == "ğ‘¦" and word[-2:] == "le":
                root = root[:-2] + "ğ‘¤"
            if root[-3:] == "ğ‘Ÿğ‘©ğ‘¥" and suff not in ["'", "ğ‘›:", "ğ‘Ÿ:", "ğ‘¦ğ‘™"]:
                root = root[:-3] + "ğ‘Ÿğ‘¥"
            if root[-2:] in ["ğ‘©ğ‘¤", "ğ‘­ğ‘¤", "ğ‘¾ğ‘¤"] and suff == "ğ‘¦ğ‘‘ğ‘¦":
                mid = "ğ‘¦" if root[-3] in "ğ‘–ğ‘—ğ‘ ğ‘¡" or root[-2] == "ğ‘¾" else ""
                root = root[:-2] + mid + "ğ‘¨ğ‘¤"
            mid = root[-1] + suff[0]
            if mid == "ğ‘¦ğ‘©":
                mid = "ğ‘¾"
            if mid == "ğ‘¦ğ‘¼":
                mid = "ğ‘½"
            if mid in ["ğ‘¤ğ‘¤", "ğ‘¯ğ‘¯"] and len(suff) < 3:
                mid = mid[0]
            best = (score, root[:-1] + mid + suff[1:])
    if long > 1 and low[-1] == low[-2] and low[-2] not in "aeiosu":
        aflag |= 2
        root = suffix_split(inp[:-1], "UNK", 0)
        if best[0] < root[0]:
            best = root
    if len(best[1]) > 1:
        word = best[1][:-2]
        end = best[1][-2:]
        if end in ["ğ‘›:", "ğ‘Ÿ:"]:
            tail = -1
            while word[tail] in ["'", ":"]:
                tail -= 1
            if word[tail] in {"ğ‘›:": "ğ‘‘ğ‘›", "ğ‘Ÿ:": "ğ‘•ğ‘–ğ‘—ğ‘Ÿğ‘ ğ‘¡"}[end]:
                word += "ğ‘©"
            elif word[tail] >= "ğ‘" and word[tail] < "ğ‘˜":
                end = chr(ord(end[0]) - 10) + ":"
        word += end
        if word[-4:] == "ğ‘’ğ‘©ğ‘¤ğ‘¦" and word[-5] in "ğ‘¦ğ‘©":
            word = word[:-4] + "ğ‘’ğ‘¤ğ‘¦"
        if word[-1] == "ğ‘¾":
            word += "0"
        if (
                word[-2] == "ğ‘¾"
                and len(word) > 4
                and word[-3] in "ğ‘‘ğ‘•ğ‘Ÿ"
                and word[-1] in "ğ‘•ğ‘¤ğ‘¯0"
        ):
            mid = "ğ‘–"
            if word[-3] == "ğ‘‘":
                if word[-4] == "ğ‘•":
                    mid = "ğ‘—"
            elif word[-1] in "ğ‘¯0" and word[-4] in "ğ‘°ğ‘±ğ‘´ğ‘µğ‘·ğ‘»ğ‘¿":
                mid = "ğ‘ "
            word = word[:-3] + mid + "ğ‘©" + word[-1]
        if word[-1] == "0":
            word = word[:-1]
        best = (best[0], word)
    return best


# aflag:
# "apostraphe_flag" : indicates apostraphe boundary.
# 1 if in a part followed by apostraphe
# 2 if in a part preceded by apostraphe


def prefix_split(word, pos, ms):
    global aflag
    best = suffix_split(word, pos, 0)
    if best[0] == len(word) ** 2:
        return best
    for split in range(len(word) - 2, ms, -1):
        pref = "^" + word[:split].lower()
        if not pref in dict:
            continue
        if word[: split + 1].lower() == "un":
            continue
        aflag = word[split - 1] != "'"
        root = prefix_split(word[split:], pos, 1)
        score = split ** 2 + root[0] if root[0] else 0
        if pref == "^la":
            score -= 4
        if score > best[0]:
            dot = "Â·" if word[0].isupper() else ""
            pref = dict[pref]
            if (
                    pref[-1] == root[1][0]
                    and pref[-1] in "ğ‘¤ğ‘¥ğ‘®ğ‘¯"
                    and pref[-2] == "ğ‘¦"
                    or pref == "ğ‘¥ğ‘©ğ‘’"
                    and (root[1][0] == "ğ‘’" or root[1][:2] == "Â·ğ‘’")
            ):
                pref = pref[:-1]
            best = (score, pref + dot + root[1])
    return best


first = True
notags = True

with importlib.resources.files('dechifro').joinpath('amer.dict.xz').open('rb') as xz:
    df = pd.read_csv(xz, header=None, names=['key', 'val'], dtype=str, na_filter=False, compression='xz', sep=' ')
    for i, (key, val) in df.iterrows():
        if notags:
            key = re.sub("_[A-Z]+", "", key)
            if key + "_" in dict:
                key += "_"
        if first and key in dict:
            if val not in dict[key].split("@"):
                dict[key] += "@" + val
        else:
            dict[key] = val
        if not first:  # Allow extra dicts to force dotting
            low = key.lower()
            if low != key and low in dict:
                del dict[low]
        if val == ".":
            del dict[key]


def isolated_lookup(word, pos='UNK'):
    global aflag
    try:
        aflag = 0
        dot = word[0].isupper()
        root = prefix_split(word, pos, 0)
        if root[1]:
            tran = root[1].replace("'", apostrophe).replace(":", "")
            dot = dot or "Â·" in tran
            tran = tran.replace("Â·", "")
            if dot:
                tran = "Â·" + tran
            return tran
        else:
            return None
    except IndexError:
        print(f'Failed to convert {word!r}', file=sys.stderr)
        return None


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage:", sys.argv[0], "file1.dict file2.dict ...")
        exit()

    for fname in sys.argv[1:]:
        if fname == "-f":  # Use Flair for POS tagging
            save = sys.stdout
            sys.stdout = sys.stderr
            from flair.data import Sentence
            from flair.models import SequenceTagger

            flair = SequenceTagger.load("flair/pos-english-fast")
            sys.stdout = save
            continue
        if fname == "-s":  # Use spaCy for POS tagging
            import spacy

            spaCy = spacy.load("en_core_web_sm")
            continue
        if fname == "-n":
            notags = True
            continue
        first = False

    text = sys.stdin.read()
    text = re.sub("([Ê»Ëˆâ€˜â€™Â´`]|&(#8217|rsquo);)", "'", text)
    text = re.sub(r"\[([a-zA-Z])\]", r"\1", text)
    text = re.sub(r"\b[Dd][Ee]-", r"dee-", text)
    text = re.sub(r"\bw/o\b", r"without", text)
    text = re.sub(r"\bw/", r"with ", text)
    text = re.sub("[Â­â€‹]", "", text)  # soft hyphen, zero-width space

    toki = 1
    parser = MyHTMLParser()
    parser.feed(text)

    tags = []
    if flair:  # Do part-of-speech tagging
        sen = []
        for tok in tokens + ["."]:
            sen.append(tok)
            if tok in [".", "?", "!"]:
                sen = Sentence(sen)
                flair.predict(sen)
                for tok in sen:
                    tags.append((tok.text, tok.get_label("pos").value))
                sen = []
        del tags[-1]
    elif spaCy:
        tags = [(w.text, w.tag_) for w in spaCy(spacy.tokens.Doc(spaCy.vocab, tokens))]
    elif notags:
        tags = [(tok, "UNK") for tok in tokens]
    else:
        import nltk

        tags = nltk.pos_tag(tokens)

    jtags = []  # Re-join broken contractions
    for token in tags:
        if token[0].lower() in contr:
            jtags[-1] = (jtags[-1][0] + token[0], jtags[-1][1] + "+" + token[1])
        else:
            jtags.append(token)

    out = []  # Translate to Shavian
    prev = (".", ".")
    toki = 1
    initial = maydot = True
    map = {"ğ‘‘": "ğ‘‘ğ‘µ", "ğ‘“": "ğ‘“ğ‘¹", "ğ‘": "ğ‘©ğ‘", "ğ‘¯": "ğ‘¨ğ‘¯ğ‘›"}
    if False == 1:
        map.update({"ğ‘©": "ğ‘±", "ğ‘©ğ‘¯": "ğ‘¨ğ‘¯", "ğ‘": "ğ‘ªğ‘"})
    else:
        map["ğ‘"] = "ğ‘ğ‘©"
    for token in jtags[1:]:
        if toki in htags:
            out.append(htags[toki])
            if htags[toki].lower().find("<div") + 1:
                initial = True
        toki += 1
        #  print (token, file=sys.stderr)
        if token[0] in [".", "?", "!", ":", '"', "â€œ", "â€"]:
            initial = True
        word = token[0]
        low = word.lower()
        befto = {"have": "ğ‘¨ğ‘“", "has": "ğ‘¨ğ‘•", "used": "ğ‘•ğ‘‘", "unused": "ğ‘•ğ‘‘", "supposed": "ğ‘•ğ‘‘"}
        if prev[0] in befto and low == "to":
            if notags:
                out.extend(["@", befto[prev[0]], " "])
            elif prev != ("used", "VBN"):  # If "to" changes the meaning of the preceding
                for i in reversed(
                        range(len(out))
                ):  # word, it also changes the pronunciation.
                    if out[i] == tran:
                        break
                out[i] = tran[:-2] + befto[prev[0]]
        tran = word
        if word[0].isalpha():
            dot = word[0].isupper()
            if initial:
                initial = False
                if len(word) == 1 or word[1].islower():
                    word = word[0].lower() + word[1:]
            aflag = 0
            root = prefix_split(word, token[1], 0)
            if root[1]:
                tran = root[1].replace("'", apostrophe).replace(":", "")
            if prev[0][0].isdigit() and low in units:
                tran = units[low]
            if not False:
                dot = maydot and "Â·" in tran
            tran = tran.replace("Â·", "")
            if False and tran in map:
                tran = map[tran]
            if False == 1:
                if tran[-2:] in ["ğ‘©ğ‘¤", "ğ‘©ğ‘¥", "ğ‘©ğ‘¯"]:
                    tran = tran[:-2] + tran[-1]
            if False == 2:
                tran = tran.replace("ğ‘£ğ‘’", "x").replace("ğ‘£ğ‘œ", "É£").replace("ğ‘£ğ‘¤", "É¬")
            if False == 3:
                for tup in [
                    ("ğ‘¦ğ‘¹", "ğ‘¦ğ‘·ğ‘®"),
                    ("ğ‘£ğ‘¿", "ğ‘£ğ‘˜ğ‘µ"),
                    ("ğ‘’ğ‘¢", "á›¢"),
                    ("ğ‘•ğ‘—", "á›¥á›„"),
                    ("ğ‘•ğ‘‘", "á›¥"),
                    ("ğ‘£ğ‘¤", "áš»\u200dá›š"),
                ]:
                    tran = tran.replace(tup[0], tup[1])
                tran = re.sub("([ğ‘¦ğ‘°][ğ‘ªğ‘´ğ‘·]|ğ‘£[ğ‘’ğ‘˜ğ‘œ])", "á›‡", tran)
            if False and low[-2:] not in ["id", "iz", "zz"]:
                for i in range(3, len(tran)):
                    if tran[i] == "ğ‘¦" and (
                            tran[i + 1:] in ["", "ğ‘›", "ğ‘Ÿ"] or tran[i + 1] == "ğ‘¦"
                    ):
                        tran = tran[:i] + "ğ’€" + tran[i + 1:]
            if dot:
                tran = "Â·" + tran
                maydot = dot_entire_name
            else:
                maydot = True
        elif word != "-":
            maydot = True  # Names may contain hyphens
        out.append(tran)
        if low != " ":
            prev = (low, token[1])
    if toki in htags:
        out.append(htags[toki])
    out = "".join(out).replace("ğ‘²ğ‘Ÿğ‘±ğ‘–ğ‘©ğ‘¯", dict["ization"][1:])
    if False:
        map = [
            [
                "ğ¹",
                "ğ»",
                "ğ¿",
                "ğ‘",
                "ğ‘ƒ",
                "ğ‘…",
                "ğ‘‡",
                "ğ½",
                "ğ·",
                "ğ‘",
                "ğº",
                "ğ¼",
                "ğ‘€",
                "ğ‘‚",
                "ğ‘„",
                "ğ‘†",
                "ğ‘ˆ",
                "ğ¾",
                "ğ¶",
                "ğ¸",
                "ğ‘Š",
                "ğ‘‹",
                "ğ®",
                "ğ¯",
                "ğ°",
                "ğ²",
                "ğ±",
                "ğ³",
                "ğµ",
                "ğª",
                "ğ‘‰",
                "ğ‘Œ",
                "ğ¨",
                "ğ©",
                "ğ´",
                "ğ²",
                "ğ¬",
                "ğ­",
                "ğ‘",
                "ğ«",
                "ğªğ‘‰",
                "ğ«ğ‘‰",
                "ğ©ğ‘‰",
                "ğ²ğ‘‰",
                "ğ²ğ‘‰",
                "ğ¨ğ‘‰",
                "ğ¨ğ²",
                "ğ‘",
                "ğ¨",
            ],
            [
                "p",
                "t",
                "k",
                "f",
                "Î¸",
                "s",
                "Êƒ",
                "tÊƒ",
                "j",
                "Å‹",
                "b",
                "d",
                "g",
                "v",
                "Ã°",
                "z",
                "Ê’",
                "dÊ’",
                "w",
                "h",
                "l",
                "m",
                "Éª",
                "É›",
                "Ã¦",
                "É™",
                "É’",
                "ÊŠ",
                "aÊŠ",
                "É‘Ë",
                "r",
                "n",
                "iË",
                "eÉª",
                "aÉª",
                "ÊŒ",
                "É™ÊŠ",
                "u",
                "É”Éª",
                "É”Ë",
                "É‘Ër",
                "É”Ër",
                "É›É™r",
                "ÉœËr",
                "É™r",
                "ÉªÉ™r",
                "ÉªÉ™",
                "ju",
                "i",
            ],
            [
                "á›ˆ",
                "á›",
                "á›£",
                "áš ",
                "áš¦",
                "á›‹",
                "á›‹áš³",
                "áš³",
                "á›„",
                "á›",
                "á›’",
                "á›",
                "áš¸",
                "áš \u200dáš ",
                "áš¦",
                "á›‰",
                "á›‰áš³",
                "áš·",
                "áš¹",
                "áš»",
                "á›š",
                "á›—",
                "á›",
                "á›–",
                "áš«",
                "á›Ÿ",
                "áš©",
                "áš¢",
                "áš£",
                "ášª",
                "áš±",
                "áš¾",
                "á›",
                "á›–á›¡",
                "ášªá›¡",
                "ášª",
                "áš©áš¢",
                "áš¢",
                "áš©á›¡",
                "áš©",
                "ášªáš±",
                "áš©\u200dáš±",
                "á›–\u200dáš±",
                "á›Ÿáš±",
                "á›Ÿáš±",
                "á› áš±",
                "á› ",
                "á›„áš¢",
                "á›",
            ],
        ][False - 1]
        cmap = {}
        if False == 3:
            cmap = {
                " ": "á›«",
                "-": "á›«",
                ",": "á›«á›«",
                ".": "á›¬",
                "!": "á›­",
                ":": "á›¬á›«",
                ";": "á›«á›¬",
                "â€¦": "á›«á›«á›«",
                "(": "[",
                ")": "]",
                "[": "(",
                "]": ")",
                "?": "?",
                "'": "",
                '"': "",
                "â€˜": "âŸ¨",
                "â€™": "âŸ©",
                "â€œ": "âŸª",
                "â€": "âŸ«",
            }
        inp = ">" + out + " "
        out = []
        squote = False
        dquote = False
        for i in range(len(inp) - 2):
            char = inp[i + 1]
            if char >= "ğ‘" and char <= "ğ’€":
                char = map[ord(char) - ord("ğ‘")]
                if inp[i] == "Â·":
                    char = char[0].upper() + char[1:]
            if inp[i] == "<":
                content = False
            if inp[i] == ">":
                content = True
            if inp[i] in "ğ‘¦ğ‘ªğ‘«ğ‘³":
                if char in "á›ˆá›‹á›’á›áš¸á›‰áš·áš¹á›—áš±áš¾":
                    char = char + "\u200d" + char
                elif char in "á›áš áš¦áš³á›š":
                    char = char + char
                elif char == "á›£":
                    char = "á›¤"
            if inp[i + 2] in "ğ‘¤ğ‘®ğ‘˜ğ‘¿á›„" and char == "á›£":
                char = "áš³"
            if content and char in cmap:
                if char == "'":
                    char = "â€˜â€™"[squote]
                    squote = not squote
                if char == '"':
                    char = "â€œâ€"[dquote]
                    dquote = not dquote
                char = cmap[char] + "\u200b"
                if inp[i] in cmap and inp[i + 1] == " ":
                    char = ""
            if char != "Â·":
                out.append(char)
        out = "".join(out)
        for tup in [
            ("á›á›", "á›¡á›"),
            ("á›á›¥á›„", "á›á›‹\u200dá›‹áš³"),
            ("áš¾\u200dáš¾á›", "áš¾áš¾á›"),
            ("áš¾á›", "áš¾\u200dá›"),
            ("áš£á›Ÿáš±", "áš£áš±"),
            ("áš¢á›Ÿáš±", "áš¢áš±"),
            ("ášªá›¡á›Ÿáš±", "ášªá›¡áš±"),
            ("á›‹áš³á›Ÿáš¾", "á›‹áš³áš¾"),
            ("á›‹áš³á›„", "á›‹á›£á›„"),
            ("á›‰áš³á›Ÿáš¾", "á›‰áš³áš¾"),
            ("á›¤á›¥", "á›¤á›‹á›"),
        ]:
            out = out.replace(tup[0], tup[1])
    print(out, end="")
